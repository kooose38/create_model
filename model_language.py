#自然言語処理model
# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10lE-2vZt3qnNcgQYdDug_DDk0x5XEN1X
"""

!pip install -q pytorch_lightning

!apt install aptitude

!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y

!pip install mecab-python3==0.7

import torch,torchvision
import pytorch_lightning as pl 
import matplotlib.pyplot as plt 
import seaborn as sns 
import numpy as np
import pandas as pd 
import MeCab
from glob import glob
from google.colab import files
import torch.nn as nn 
import torch.nn.functional as F 
from torchvision import transforms

from pytorch_lightning.metrics.functional import accuracy

uploaded=files.upload()

!unzip -d text texts.zip
!ls text/

direction=glob('/content/text/*')
direction

paths=glob('{}/*.txt'.format(direction[0]))
paths

#fileの読み込み
def get_texts_labels():
  direction=glob('/content/text/*')
  texts=[]
  labels=[]

  for i,dir in enumerate(direction):
    paths=glob('{}/*.txt'.format(dir))
    for path in paths:
      with open(path,encoding='utf-8') as f:
        text=''.join(f.readlines()[2:])
        texts.append(text)
        labels.append(i)

  return texts,labels

texts,labels=get_texts_labels()

def get_nouns(texts):
  mecab=MeCab.Tagger('-Ochasen')
  res=mecab.parse(texts)

  words=res.split('\n')[:-2]
  nouns=[]
  for word in words:
    part=word.split('\t')
    if '名詞' in part[3]:
      nouns.append(part[0])
  return nouns

words=[]
for text in texts:
  nouns=get_nouns(text)
  words.append(' '.join(nouns))

from sklearn.feature_extraction.text import CountVectorizer

conv=CountVectorizer(min_df=20)
x=conv.fit_transform(words)
print(x[0])

x=x.toarray()
print(x)

x.shape

conv.vocabulary_

df=pd.DataFrame(x)
df['target']=np.array(labels)
df

df['target'].value_counts()

x=torch.tensor(x,dtype=torch.float32)
t=torch.tensor(labels,dtype=torch.int64)

dataset=torch.utils.data.TensorDataset(x,t)

n_train=int(len(dataset)*0.6)
n_val=int(len(dataset)*0.2)
n_test=len(dataset)-n_train-n_val

torch.manual_seed(0)
pl.seed_everything(0)
train,val,test=torch.utils.data.random_split(dataset,[n_train,n_val,n_test])

len(train),len(val),len(test)

batch_size=128

train_loader=torch.utils.data.DataLoader(train,batch_size,shuffle=True,drop_last=True)
val_loader=torch.utils.data.DataLoader(val,batch_size)
test_loader=torch.utils.data.DataLoader(test,batch_size)

len(train_loader)

class Net(pl.LightningModule):
  def __init__(self):
    super(Net,self).__init__()
    self.fc1=nn.Linear(5795,200)
    self.fc2=nn.Linear(200,8)
    self.dropout=nn.Dropout2d(p=0.5)

  def forward(self,x):
    x=self.fc1(x)
    x=self.dropout(x)
    x=F.relu(x)
    x=self.fc2(x)
    x=F.relu(x)
    return x

  def training_step(self,batch,batch_idx):
    x,t=batch
    y=self.forward(x)
    loss=F.cross_entropy(y,t)
    return loss

  def validation_step(self,batch,batch_idx):
    x,t=batch
    y=self.forward(x)
    loss=F.cross_entropy(y,t)
    acc=accuracy(y,t)
    self.log('val_loss',loss)
    self.log('val_acc',acc)
    return loss

  def test_step(self,batch,batch_idx):
    x,t=batch
    y=self.forward(x)
    loss=F.cross_entropy(y,t)
    acc=accuracy(y,t)
    self.log('test_loss',loss)
    self.log('test_acc',acc)
    return loss

  def configure_optimizers(self):
    optimizer=torch.optim.Adam(self.parameters(),lr=0.01,weight_decay=0.001)
    return optimizer

net=Net()
trainer=pl.Trainer(max_epochs=10,gpus=1)
trainer.fit(net,train_loader,val_loader)

trainer.test(test_dataloaders=test_loader)

trainer.callback_metrics

# Commented out IPython magic to ensure Python compatibility.
%load_ext tensorboard
%tensorboard --logdir lightning_logs/

net.state_dict().keys()

torch.save(net.state_dict(),'sample.pt')

net=Net()
net.load_state_dict(torch.load('sample.pt'))
a=x[3001].reshape(1,5795)
y=net.forward(a)
torch.argmax(y)

labels[3001]

